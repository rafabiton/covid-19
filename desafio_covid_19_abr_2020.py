# -*- coding: utf-8 -*-
"""Desafio-Covid-19-Abr-2020.ipynb

Automatically generated by Colaboratory.
    

# **Importando das Bibliotecas gerais**
"""

import numpy as np 
import zipfile
import pandas as pd
import glob
import json
import seaborn as sns
import spacy
import nltk
from IPython.core.display import HTML
from matplotlib import pyplot as plt

!pip install scispacy

import scispacy

!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz

import en_core_sci_md

!python -m spacy download en

"""Carregando dos Arquivos"""

from google.colab import drive
drive.mount('/content/gdrive')

path = '/content/gdrive/My Drive/CORD-19-research-challenge.zip'
zip_object = zipfile.ZipFile(file=path, mode = 'r')
zip_object.extractall('./covid19')
zip_object.close()

"""# **Criação do DataFrame com os textos**"""

corona_features = {'paper_id':[], 'title':[], 'abstract':[], 'text':[]}

type(corona_features)

corona_df = pd.DataFrame.from_dict(corona_features)
type(corona_df)

corona_df.head()

json_filenames = glob.glob(f'{"./covid19"}//**/*.json', recursive=True)

print(json_filenames)

len(json_filenames)

def return_corona_df(json_filenames, df):
  for file_name in json_filenames:
    row = {'paper_id': None, 'title': None,
           'abstract': None, 'text': None}
    
    with open(file_name) as json_data:
      if file_name == './sample_data/anscombe.json':
        continue
 
      data = json.load(json_data)      
      
      if 'paper_id' not in data:
        row['paper_id'] = ''       
      else:       
        row['paper_id'] = data['paper_id'].strip()
       
      row['title'] = data['metadata']['title'].strip()
      if 'abstract' not in data:
        row['abstract'] = ''
      else:
        abstract_list = [abstract['text'] for abstract in data['abstract']]
        abstract = '\n '.join(abstract_list)
        row['abstract'] = abstract.strip()      
 
      text_list = [text['text'] for text in data['body_text']]
      text = '\n '.join(text_list)
      row['text'] = text.strip()
      
      df = df.append(row, ignore_index = True)
  return df

corona_df = return_corona_df(json_filenames, corona_df)

corona_df.shape

corona_df.head()

"""# Pré-Processamento

Valores Faltantes (geramos um gráfico para mostrar os campos vazios. Quanto mais próximo de zero, significa que as variáveis não possuem valores nulos ou vazios
"""

sns.heatmap(corona_df.isnull());

corona_df.shape

len(corona_df[corona_df['paper_id'] == ''])

len(corona_df[corona_df['title'] == ''])

len(corona_df[corona_df['abstract'] == ''])

len(corona_df[corona_df['text'] == ''])

"""Retiramos os artigos que contem titulo e abastract vazios do DF corona_df"""

corona_df = corona_df[corona_df['title'] != '']

corona_df = corona_df[corona_df['abstract'] != '']

corona_df.shape

"""Remover valores duplicados. Inplace=True ele já faz a remoção dos itens duplicasdos e já aplica ao DF"""

corona_df.drop_duplicates(['abstract','text','title'], inplace = True)

corona_df.shape

"""# Amostra da Base Dados

random_state = 1 é para ele sempre trazer a mesma amostra de dados, indepedente do momento da execução
"""

corona_df = corona_df.sample(n=500, random_state=1)

corona_df.shape

corona_df.head()

corona_df.tail()

"""Extraímos o texto de uma das amostras (neste caso escolhi o primeiro artigo da amostra, mas poderia ser qualquer um)"""

sample_text = corona_df['text'][22895]
sample_text

"""# Função para Pré-processamento
Vamos criar uma função para pré-processar os dados
"""

# utilizaremos a classe padrão do Spacy, pois já é preparada para área médica
# disable para agilizar o processamento dos textos
#tagger indica o pipeline, parser vai fazer o parse de independencias e ner é o reconhecimento de entidades no texto, mas nao utilizaremos nenhuma das 3
nlp = en_core_sci_md.load(disable=['tagger', 'parser','ner'])
nlp.max_length = 2000000

# vamos visualizar as stop words: Stop Words são palavras que não usaremos, palavras mais do cotidiano (meu, seu, onde, então, etc)
print(spacy.lang.en.stop_words.STOP_WORDS)

# Ao criarmos a nuvem de palavras, identificamos algumas palavras que não são necessárias, como et e al, por exemplo
# por este motivo, vamos criar uma nova STOP WORDS, removendo estas palavras
new_stop_words = ['et','al','doi','copyright','http','https','fig','table','result','show']
for word in new_stop_words:
  nlp.vocab[word].is_stop=True

def spacy_tokenizer(sentence):
  sentence = sentence.lower()
  #extrair o lemma das palavras
  list = []
  list = [word.lemma_ for word in nlp(sentence) if not (word.is_stop or
                                                        word.is_punct or
                                                        word.like_num or
                                                        word.is_space or
                                                        len(word)==1)]
  list = ' '.join([str(element) for element in list])
  return list

sample_text

test = sample_text
result = spacy_tokenizer(test)
result

corona_df['text'] = corona_df['text'].apply(spacy_tokenizer)

print(sample_text)

len(sample_text)

print(corona_df['text'][22895])

len(corona_df['text'][22895])

"""# Termos Frequentes
Para isso, vamos usar a biblioteca NLTK
"""

# vamos criar uma pasta chamada corpus e dentro dela vamos colocar todos os artigos já tratados, salvos em txt
for index, row in corona_df.iterrows():
  print(row['paper_id'], row['title'])
  text_file = open('./covid19/corpus/' + row['paper_id'] + '.txt', 'w')
  # vamos colocar o conteúdo dentro do arquivo
  n = text_file.write(row['text'])
  text_file.close()

# Vamos passar para o NLTK para ele fazer a leitura dos arquivos
from nltk.corpus import PlaintextCorpusReader
corpus = PlaintextCorpusReader('covid19/corpus','.*')

files = corpus.fileids()
files[0]

corpus.raw('0080d3bd9fb92e022c27715c2d1249042aa998b8.txt')

"""# Montando a nuvem de palavras (palavras mais frequentes)"""

# nuvem de palavras
words = corpus.words()
print(words)

len(words)

# Vamos usar a função FreqDist do NLTK para mostrarmos a frequencia das 100 palavras mais citadas
frequency = nltk.FreqDist(words)
most_common = frequency.most_common(100)
most_common

# Montando a nuvem de palavras através da biblioteca map
from matplotlib.colors import ListedColormap
color_map = ListedColormap(['orange','green','red','magenta'])

from wordcloud import WordCloud
cloud = WordCloud(background_color = 'white', max_words=100, colormap=color_map)

cloud = cloud.generate(corona_df['text'].str.cat(sep='\n'))
plt.figure(figsize=(15,15))
plt.imshow(cloud)
plt.axis('off')
plt.show

# salvar o arquivo
corona_df.to_csv('covid19/corona_df.csv')

"""# Extração de Entidades Nomeadas

Testes com a função
"""

text = str(corona_df['text'][22895])
print(text)

nlp_ent = spacy.load('en')
nlp_ent.max_length = 2000000

# documentacao do Spacy: spacy.io/api/annotation
doc = nlp_ent(text)

# codigo do spacy, vai recuperar todas as entidades da nossa variavel doc
# Vamos filtrar apenas o que é País e nacionalidade (que é uma das perguntas do desafio do Kaggle)
for entity in doc.ents:
  if entity.label_ == 'NORP' or entity.label_ == 'GPE':
    print(entity.text, entity.label_)

"""Vamos melhorar a forma de visualizar estes dados"""

from spacy import displacy
displacy.render(doc, style='ent', jupyter=True)

"""Contagem das entidades na base de dados"""

# Vamos verificar quais os países mais citados nestes artigos
gpe = []
for index, row in corona_df.iterrows():
  text = row['text']
  doc = nlp_ent(text)
  for entity in doc.ents:
    if entity.label_ == 'GPE':
      gpe.append(str(entity.text))

print(gpe)

values_gpe, counts_gpe = np.unique(np.array(gpe), return_counts= True )

values_gpe, counts_gpe

gpe_df = pd.DataFrame({'paises': values_gpe, 'counts': counts_gpe})
gpe_df.head()

gpe_df.shape

gpe_df_filtered = gpe_df[gpe_df.counts > 50]

gpe_df_filtered.shape

gpe_df_filtered.head(16)

# Vamos gerar um gráfico de barras dos paises que mais possuem referencia nos documentos
sns.set(rc={'figure.figsize':(15,8)})
sns.barplot(x='paises', y='counts', hue='paises', data=gpe_df_filtered);

"""# Pesquisa com uma palavra: Um dos itens do desafio do Kaggle é poder pesquisar em todos os artigos por uma palavra (smoking, por exemplo), para isso, vamos usar NLTK"""

# Estamos colocando dentro da variavel text, todos os textos contidos na variável corpus
# Usei o NLTK aqui e a funcao concordance para mostrar uma forma bem simples de fazer busca de palavra nos artigos, porém
# esta funcao tem algumas limitações, por isso, vamos criar uma funcao mais robusta utilizando o find() do Python
text = nltk.Text(corpus.words())

# criamos esta variavel match para que ela receba e faca a concordancia do termo buscado na variavel text
match = text.concordance('pulmonary', width= 150, lines=30)

"""# Pesquisa com uma palavra utilizando FIND"""

# Codigo baseado em: https://jornaldev.com/23666/python-string-find
def find_texts(input_str, search_str, number_of_words):
  l = []
  index = 0
  number_of_words = number_of_words
  while index < len(input_str):
    i = input_str.find(search_str, index)
    # se a palavra não for localizada
    if i == -1:
      return l

    if input_str [i-number_of_words:i] == '':
      start = 0
    else:
      start = i - number_of_words

    l.append(input_str[start:i] + input_str[i:i+number_of_words])
    index = i + 1
  return l

# vamos testar a funcao para ver se ela faz o que queremos
string = 'cost time well knowledge report tq-pcr assay differential detection hadv serotype tq-pcr reveal stable repeatability sensitivity copies/reaction cross reaction common respiratory virus hadv'
search_string = 'time'
texts = find_texts(string, search_string, 15)
texts

# Vamos visualizar os dados em HTML
# Realcando a palavra que estamos buscando no texto
display(HTML(f'<h1> {search_string.upper()} </h1>'))
display(HTML(f"""<p><strong>Number of Matches: </strong>{len(texts)}</p>"""))
for i in texts:
  marked_text = str(i.replace(search_string,f"<mark>{search_string}</mark>"))
  display(HTML(f"""<blockquote>...{marked_text}...</blockquote>"""))

search_string = 'pulmonary disease'

# Foi feita a lematização, ou seja, ele lematizou Smoking para Smoke
# Por isso é importante utilizar o spacy_tokenizer
search_string = spacy_tokenizer(search_string)
search_string

# Funcao baseada na anterior
def find_all_texts(input_str, search_str, number_of_words):
  text_list = []
  index = 0
  number_of_words = number_of_words
  while index < len(input_str):
    i = input_str.find(search_str, index)
    # se a palavra não for localizada
    if i == -1:
      return text_list

    if input_str [i-number_of_words:i] == '':
      start = 0
    else:
      start = i - number_of_words

    text_list.append(input_str[start:i] + input_str[i:i+number_of_words])
    index = i + 1
  return text_list

documents = []
for index, row in corona_df.iterrows():
  documents.append(find_all_texts(row['text'], search_string, 40))

len(documents)

for doc in documents:
  if doc != []:
    print(doc)

# Vamos buscar em cada artigo a palavra chave (nesse caso Smoke) e trazer também
# o ID do artigo e o número de ocorrencias
for index, row in corona_df.iterrows():
  texts = find_all_texts(row['text'], search_string, 400)
  if texts == []:
    continue

  paper_id = row['paper_id']
  title = row['title']
  display(HTML(f'<h1> {search_string.upper()} </h1>'))
  display(HTML(f"""<p>
                      <strong>Title:</strong> {title}</br>
                      <strong>Id:</strong> {paper_id}</br>
                      <strong>Number of Matches: </strong>{len(texts)}
                   </p>"""))
  for i in texts:
    marked_text = str(i.replace(search_string,f"<mark>{search_string}</mark>"))
    display(HTML(f"""<blockquote>...{marked_text}...</blockquote>"""))

"""O desafio pede vantagens e desvantagens na implementação destes métodos.

Vantagens:

*   Rápido
*   Fácil implementação
*   Muito útil para pesquisas simples com uma palavra
*   Bom para palavras chaves

Desvantagens:

*   Busca somente uma palavra
*   Não possui ordenação de importância
*   Considera somente a palavra "completa"

# Pesquisa com mais palavras, utilizando Spacy
"""

string = 'cost time well knowledge report tq-pcr assay differential detection hadv serotype tq-pcr reveal stable repeatability sensitivity copies/reaction cross reaction common respiratory virus hadv'

search_strings = ['detection', 'reveal']
# vai receber o Natural Language Process
tokens_list = [nlp(item) for item in search_strings]

tokens_list

# Vamos associar as palavras do matcher com as do modelo de Vocab do Spacy
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
matcher.add('SEARCH', None, *tokens_list)

doc = nlp(string)
matches = matcher(doc)
matches
# O retorno indica: ID da busca, posição de início e posição de término da palavra buscada no texto

doc[8:9]

doc[12:13]

# Para trazer mais palavras a esquerda e a direita
doc[8-5:8+5]

# para acessar o índice onde inicia e onde termina a posição das palavras
matches[0][1]

matches[0][2]

doc[matches[0][1]:matches[0][2]]

"""# Vamos testar o Spacy"""

search_strings = ['smoking', 'pulmonary disease']
tokens_list = [nlp(spacy_tokenizer(item)) for item in search_strings]
tokens_list

from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
matcher.add('SEARCH', None, *tokens_list)
number_of_words = 50

search_string_html = ' '.join([str(element) for element in search_strings])
search_string_html

for index, row in corona_df.iterrows():
  marked_text = ''
  doc = nlp(row['text'])
  paper_id = row['paper_id']
  title = row['title']
  matches = matcher(doc)
  if matches == []:
    continue
  display(HTML(f'<h1> {search_string_html.upper()} </h1>'))
  display(HTML(f"""<p>
                      <strong>Title:</strong> {title}</br>
                      <strong>Id:</strong> {paper_id}</br>
                      <strong>Number of Matches: </strong>{len(matches)}
                   </p>"""))
  for i in matches:
    start = i[1] - number_of_words
    if start < 0:
      start = 0
    for j in range(len(tokens_list)):
      if doc[i[1]:i[2]].similarity(tokens_list[j]) == 1.0:
        search_text = str(tokens_list[j])
        marked_text += str(doc[start:i[2] + number_of_words]).replace(search_text, f"<mark>{search_text}</mark>")
        marked_text += "<br /><br />"
    display(HTML(f"""<blockquote>...{marked_text}...</blockquote>"""))

"""# Similaridades entre strings com fuzzywuzzy


*   Distância Levenshtein (https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_Levenshtein)

Vamos testar o fuzzywuzzy
"""

# Vamos instalar a biblioteca
!pip install fuzzywuzzy
!pip install python-Levenshtein

# vamos importar a lib
from fuzzywuzzy import fuzz

# similaridade entre strings
fuzz.ratio('Apple Inc.', 'Apple')
# O resultado significa o grau de similaridade entre as strings (em uma escala de 0 a 100)

# similaridade da string parcial
fuzz.partial_ratio('Apple Inc.', 'Apple')

# Ignora a ordem das palavras
# Isto é importante para, caso o médico ou a pessoa esteja pesquisando sobre sintomas e os sintomas estão antes ou depois de uma palavra
fuzz.token_sort_ratio('Lakers X Chicago Bulls', 'Chicago Bulls x Lakers')

# Ingnorar palavras duplicadas
fuzz.token_set_ratio('Today we have a great game: Lakers x Chicago Bulls', 'Chicago Bulls x Lakers')

"""# Comparação com o texto completo
O texto solicitado no desafio é:
Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.
"""

search_string = 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.'

search_string = spacy_tokenizer(search_string)
print(search_string)

# vamos comparar cada um dos 500 textos (nossa amostra) com o texto acima.
ratio = []
partial_ratio = []
sort_ratio = []
set_ratio = []
for index, row in corona_df.iterrows():
  ratio.append(fuzz.ratio(row['text'], search_string))
  partial_ratio.append(fuzz.partial_ratio(row['text'], search_string))
  sort_ratio.append(fuzz.token_sort_ratio(row['text'], search_string))
  set_ratio.append(fuzz.token_set_ratio(row['text'], search_string))

# vamos verificar agora o percentual de similaridade do texto pesquisado nos artigos, para cada um dos usos do fuzz
# O resultado é um percentual de similaridade que vai de 0 a 100
np.array(ratio).mean()

np.array(partial_ratio).mean()

np.array(sort_ratio).mean()

np.array(set_ratio).mean()

"""# Comparação com Abstract
A ideia é verificar se com o Abstract, o resultado é melhor
"""

corona_df.head()

Vamos pré-processar o atributo abstract

corona_df['abstract'] = corona_df['abstract'].apply(spacy_tokenizer)

corona_df.head()

# vamos comparar cada um dos 500 textos (nossa amostra) com o texto acima.
ratio = []
partial_ratio = []
sort_ratio = []
set_ratio = []
for index, row in corona_df.iterrows():
  ratio.append(fuzz.ratio(row['abstract'], search_string))
  partial_ratio.append(fuzz.partial_ratio(row['abstract'], search_string))
  sort_ratio.append(fuzz.token_sort_ratio(row['abstract'], search_string))
  set_ratio.append(fuzz.token_set_ratio(row['abstract'], search_string))

np.array(ratio).mean()

np.array(partial_ratio).mean()

np.array(sort_ratio).mean()

np.array(set_ratio).mean()

"""# Retorno dos artigos mais similares"""

scores = {}
for index, row in corona_df.iterrows():
  scores[row['paper_id']] = fuzz.token_set_ratio(row['text'], search_string)

print(scores)

import operator 
sorted_scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)
sorted_scores[0:20]

pd.set_option('display.max_colwidth', -1)
display(HTML(f'<h3> {search_string.upper()} </h3>'))
for i in sorted_scores[0:10]:
  df = corona_df.loc[corona_df['paper_id'] == i[0]]

  display(HTML(f"""<p>
                      <strong>Title:</strong> {df.title}</br>
                      <strong>Id:</strong> {i[0]}</br>
                      <strong>Score:</strong> {i[1]}</br>
                      <strong>Abstract:</strong>{str(df.abstract)[0:800]}
                   </p>"""))

